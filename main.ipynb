{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGO Project\n",
    "## PyCBC GW Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:53:56.394718Z",
     "start_time": "2018-12-03T13:53:55.638569Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyCBC.libutils: pkg-config call failed, setting NO_PKGCONFIG=1\n"
     ]
    }
   ],
   "source": [
    "# Filter the LIGO .gwf data file using the pyCBC software package\n",
    "# Based on Alex Nitz's sample code\n",
    "# Some lines added by Hao Liu to parameterize the program and for outputs\n",
    "\n",
    "directory = 'Data/' # directory where the data files are located\n",
    "suffix = '_LOSC_4_V2-1126259446-32' # suffix of the file name, not including \".gwf\"\n",
    "match_low_frequency_cutoff = 43 # low frequency cutoff for matching, normally chosen to be same as highpass_freq\n",
    "highpass_freq = 15 # frequency for the initial high pass (Hz)\n",
    "highpass_edge_cut_left = 16 # time domain left-side cutoff after initial high pass (in second)\n",
    "highpass_edge_cut_right = 16 # time domain right-side cutoff after initial high pass (in second)\n",
    "snr_edge_cut = 8 # length of edge-cut at both ends for max-SNR determination. 2016\n",
    "psd_segment_length = 4 # segment length for calculating the PSD by Welch's averaging 4\n",
    "\n",
    "import lal as _lal\n",
    "from pycbc.frame import read_frame\n",
    "from pycbc.filter import highpass\n",
    "from pycbc.psd import interpolate, inverse_spectrum_truncation\n",
    "from pycbc.types.timeseries import load_timeseries\n",
    "from pycbc.types.frequencyseries import load_frequencyseries\n",
    "from pycbc.types.timeseries import TimeSeries\n",
    "from pycbc.waveform import get_td_waveform\n",
    "from pycbc.filter import matched_filter\n",
    "from pycbc.waveform import apply_fseries_time_shift\n",
    "from pycbc.filter import sigma\n",
    "\n",
    "%matplotlib notebook\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import readligo as rl\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import tukey\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import skew, kurtosis\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "from scipy.signal import butter, filtfilt, iirdesign, zpk2tf, freqz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:53:56.827205Z",
     "start_time": "2018-12-03T13:53:56.618907Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to read and high-pass the data, will cut some edge after that.\n",
      " \n",
      "****************************************************\n",
      "H-file is: Data/H-H1_LOSC_4_V2-1126259446-32.gwf\n",
      "H-file is from 1126259446 to 1126259478, duration: 32.0\n",
      "H-strain after high-pass is now from 1126259446 to 1126259478, duration: 32.0\n",
      " \n",
      "****************************************************\n",
      "L-file is: Data/L-L1_LOSC_4_V2-1126259446-32.gwf\n",
      "L-file is from 1126259446 to 1126259478, duration: 32.0\n",
      "L-strain after high-pass is now from 1126259446 to 1126259478, duration: 32.0\n",
      "<class 'pycbc.types.timeseries.TimeSeries'>\n"
     ]
    }
   ],
   "source": [
    "print('Start to read and high-pass the data, will cut some edge after that.')\n",
    "\n",
    "t_slice = 64\n",
    "win = tukey(32*4096, alpha=4./32.)\n",
    "\n",
    "strain, strain_i, stilde = {}, {}, {}\n",
    "for ifo in ['H', 'L']:\n",
    "    # Read the detector data\n",
    "    fname = directory + '%s-%s1' % (ifo, ifo) + suffix + '.gwf'\n",
    "    channel_name = '%s1:LOSC-STRAIN'  % ifo\n",
    "    strain_i[ifo] = read_frame(fname, channel_name)\n",
    "    strain[ifo] = strain_i[ifo] #.time_slice(1126259462-t_slice/2,1126259462+t_slice/2)\n",
    "\n",
    "    print(' ')\n",
    "    print('****************************************************')\n",
    "    print(\"%s-file is: %s\" %(ifo, fname))\n",
    "    print(\"%s-file is from %s to %s, duration: %s\" \n",
    "        %(ifo, strain[ifo].start_time, strain[ifo].end_time, strain[ifo].duration) )\n",
    "\n",
    "    # save a copy of the unfiltered strain data to disk\n",
    "    #strain[ifo] = strain[ifo].crop(highpass_edge_cut_left, highpass_edge_cut_right)\n",
    "    #strain[ifo].save('Results/'+'%s' % (ifo) + '_STRAIN_PYCBC_UNFILT_SEG_%s.npy' %(psd_segment_length))\n",
    "\n",
    "    # Initial high pass to remove the strong low-frequency signal\n",
    "    #strain_i[ifo] = highpass(strain_i[ifo], highpass_freq) \n",
    "    strain[ifo] = highpass(strain[ifo], highpass_freq)\n",
    "\n",
    "    # Edge-cut to remove time corrupted by the high pass filter\n",
    "    #strain_i[ifo] = strain_i[ifo].crop(highpass_edge_cut_left, highpass_edge_cut_right)\n",
    "    #strain[ifo] = strain[ifo].crop(highpass_edge_cut_left, highpass_edge_cut_right)\n",
    "    strain[ifo]._data = strain[ifo]._data*win\n",
    "    \n",
    "    #strain[ifo].save('Results/'+'%s' % (ifo) + '_STRAIN_PYCBC_FILT_SEG_%s.npy' %(psd_segment_length))\n",
    "\n",
    "    # Print some information\n",
    "    print(\"%s-strain after high-pass is now from %s to %s, duration: %s\" \n",
    "        %(ifo, strain[ifo].start_time, strain[ifo].end_time, strain[ifo].duration) )\n",
    "\n",
    "    # Also create a frequency domain version of the data\n",
    "    stilde[ifo] = strain[ifo].to_frequencyseries()\n",
    "\n",
    "print(type(strain['H']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:53:58.498460Z",
     "start_time": "2018-12-03T13:53:58.365352Z"
    },
    "code_folding": [
     32
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the PSD from cut strain...\n",
      "Using PyCBC method.\n",
      "PSD ready.\n",
      "<class 'pycbc.types.timeseries.TimeSeries'>\n"
     ]
    }
   ],
   "source": [
    "print('Calculate the PSD from cut strain...')\n",
    "\n",
    "psd_method = \"pycbc\" # Either 'pycbc' or 'losc'\n",
    "psds = {}\n",
    "\n",
    "if psd_method == \"pycbc\":\n",
    "    print('Using PyCBC method.')\n",
    "else:\n",
    "    print('Using LOSC method.')\n",
    "\n",
    "for ifo in ['H', 'L']:\n",
    "    \n",
    "    if psd_method == \"pycbc\":\n",
    "        # Calculate the PSD by a Welch-style estimator (with the pyCBC timeseries.psd() method)\n",
    "        # Then interpolate the PSD to the desired frequency step.\n",
    "        \n",
    "        psd_window = np.blackman(psd_segment_length*4096)\n",
    "        psds[ifo] = interpolate(strain[ifo].psd(psd_segment_length, avg_method='median', window='hann'),\n",
    "                                stilde[ifo].delta_f)\n",
    "        \n",
    "        # Smooth to the desired corruption length\n",
    "        psds[ifo] = inverse_spectrum_truncation(psds[ifo], \n",
    "                                                psd_segment_length * strain[ifo].sample_rate,\n",
    "                                                low_frequency_cutoff=43,\n",
    "                                                trunc_method='hann')\n",
    "        \n",
    "        # Whiten strain\n",
    "        strain[ifo] = (stilde[ifo] / psds[ifo]**0.5 ).to_timeseries()\n",
    "        \n",
    "        # Save PSD\n",
    "        psds[ifo].save('Results/%s' %(ifo) + '_PSD_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "    \n",
    "    elif psd_method == \"losc\":\n",
    "        # Calculate the PSD by a Welch-style estimator (with the mlab.psd() function)\n",
    "        # Then interpolate, using numpy's interp function, to the desired frequency step.\n",
    "        \n",
    "        fs = strain[ifo].sample_rate\n",
    "        NFFT = psd_segment_length*fs # 4\n",
    "        psd_window = np.blackman(NFFT)\n",
    "        NOVL = NFFT/2\n",
    "        \n",
    "        Pxx, freqs = mlab.psd(strain[ifo], Fs = fs, NFFT = NFFT, window=psd_window, noverlap=NOVL)\n",
    "        datafreqs = stilde[ifo].sample_frequencies\n",
    "        psd_tmp = np.interp(datafreqs, freqs, Pxx)\n",
    "        \n",
    "        # Save PSD\n",
    "        np.save('Results/'+'%s' %(ifo) + '_PSD_LOSC_M_SEG_%s.npy' %(psd_segment_length),\n",
    "                np.column_stack((datafreqs,psd_tmp)))\n",
    "    \n",
    "        psds[ifo] = load_frequencyseries('Results/'+'%s' % (ifo) + '_PSD_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "        \n",
    "        strain[ifo] = (stilde[ifo] / psds[ifo]**0.5 ).to_timeseries()\n",
    "\n",
    "print('PSD ready.')\n",
    "print(type(strain['H']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:53:59.953703Z",
     "start_time": "2018-12-03T13:53:59.507416Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start matching: \n",
      " \n",
      "****************************************************\n",
      "H: Consider SNR only from 1126259446 to 1126259478, duration: 32.0\n",
      "H-SNR: 17.80919189676082\n",
      "H-time: 1126259462.361083984\n",
      "H-phase: -2.7830965899354703\n",
      " \n",
      "****************************************************\n",
      "L: Consider SNR only from 1126259446 to 1126259478, duration: 32.0\n",
      "L-SNR: 12.97645835153095\n",
      "L-time: 1126259462.354003906\n",
      "L-phase: 0.5876307852351351\n",
      "Network SNR: 22.035330434660345\n"
     ]
    }
   ],
   "source": [
    "print('Start matching: ')\n",
    "\n",
    "hp, hc = get_td_waveform(   approximant=\"SEOBNRv4\", \n",
    "                            mass1=41.743, mass2=29.237, \n",
    "                            spin1z = 0.355,      spin2z = -0.769, \n",
    "                            f_lower=match_low_frequency_cutoff, delta_t = strain[ifo].delta_t)\n",
    "\n",
    "hp.append_zeros(800)\n",
    "hc.append_zeros(800)\n",
    "\n",
    "window = tukey(len(hp), alpha=0.3)\n",
    "hp._data = hp._data*window\n",
    "hp.resize(len(strain[ifo]))\n",
    "\n",
    "hp = hp.to_frequencyseries(delta_f=stilde[ifo].delta_f)\n",
    "\n",
    "max_snr, max_time, max_phase = {}, {}, {}\n",
    "for ifo in ['H', 'L']:\n",
    "    snr = matched_filter(hp, stilde[ifo], psd=psds[ifo], low_frequency_cutoff=match_low_frequency_cutoff)\n",
    "    #snr = snr.crop(snr_edge_cut, snr_edge_cut)\n",
    "    \n",
    "    _, idx = snr.abs_max_loc()\n",
    "      \n",
    "    # The complex SNR at the peak\n",
    "    max_snr[ifo] = snr[idx]\n",
    "        \n",
    "    # The time of the peak\n",
    "    max_time[ifo] = float(idx) / snr.sample_rate + snr.start_time\n",
    "    max_phase[ifo] = np.angle(max_snr[ifo])\n",
    "\n",
    "    print(' ')\n",
    "    print('****************************************************')\n",
    "    print('%s: Consider SNR only from %s to %s, duration: %s' %(ifo, snr.start_time, snr.end_time, snr.duration))\n",
    "    print('%s-SNR: %s'   %(ifo, np.absolute(max_snr[ifo])))\n",
    "    print('%s-time: %s'  %(ifo, max_time[ifo]))\n",
    "    print('%s-phase: %s' %(ifo, max_phase[ifo]))\n",
    "\n",
    "tmax, hmax, smax = max_time, hp, max_snr\n",
    "\n",
    "network_snr = (abs(np.array(max_snr.values())) ** 2.0).sum() ** 0.5\n",
    "print('Network SNR: %s' %(network_snr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:54:03.549190Z",
     "start_time": "2018-12-03T13:54:01.315523Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H relative amplitude: 0.13463021934872837\n",
      "Bandpassing and slicing.\n",
      "L relative amplitude: 0.11110052591995682\n",
      "Bandpassing and slicing.\n"
     ]
    }
   ],
   "source": [
    "i_c = 2*4096\n",
    "nq = 4096./2.\n",
    "b, a = butter(4, [43./nq, 300./nq], btype = 'bandpass')\n",
    "wt, h = {}, {}\n",
    "for ifo in ['H', 'L']:\n",
    "    # Shift the template to the maximum time at this sample rate\n",
    "    dt =  tmax[ifo] - stilde[ifo].start_time\n",
    "    inj = apply_fseries_time_shift(hmax, dt)\n",
    "\n",
    "    # Scale the template to the SNR and phase\n",
    "    inj /= sigma(hmax, psd=psds[ifo], low_frequency_cutoff=match_low_frequency_cutoff)\n",
    "    inj *= smax[ifo]\n",
    "\n",
    "    amp1 = smax[ifo] / sigma(hmax, psd=psds[ifo], low_frequency_cutoff=match_low_frequency_cutoff)\n",
    "    amp1 = np.absolute(amp1)\n",
    "    print('%s relative amplitude: %s' %(ifo, amp1*100))\n",
    "\n",
    "    # Subtract from the data\n",
    "    stilde2 = stilde[ifo] - inj\n",
    "    stilde_tmp = stilde[ifo]\n",
    "    \n",
    "    # Whiten the data\n",
    "    hoft = (stilde2 / psds[ifo] ** 0.5).to_timeseries()\n",
    "    wh = (inj / psds[ifo] ** 0.5).to_timeseries()\n",
    "    \n",
    "    h[ifo] = wh\n",
    "    wt[ifo] = hoft\n",
    "    \n",
    "    print('Bandpassing and slicing.')\n",
    "    \n",
    "    #strain[ifo] = filtfilt(b, a, strain[ifo])\n",
    "    #wt[ifo] = filtfilt(b, a, wt[ifo])\n",
    "    #h[ifo] = filtfilt(b, a, h[ifo])\n",
    "    \n",
    "    #strain[ifo] = strain[ifo][i_c:-i_c]\n",
    "    #wt[ifo] = wt[ifo][i_c:-i_c]\n",
    "    #h[ifo] = h[ifo][i_c:-i_c]\n",
    "    \n",
    "    if psd_method == 'pycbc':\n",
    "        \n",
    "        # Save a copy of the strain to disk\n",
    "        np.save('Results/'+'%s' % (ifo) + '_STRAIN_PYCBC_M_SEG_%s.npy' %(psd_segment_length), strain[ifo])\n",
    "        \n",
    "        # save a copy of the residual strain to disk\n",
    "        np.save('Results/'+'%s' % (ifo) + '_RESIDUAL_PYCBC_M_SEG_%s.npy' %(psd_segment_length), wt[ifo])\n",
    "        \n",
    "        # save a copy of the template to disk\n",
    "        np.save('Results/'+'%s' % (ifo) + '_TEMPLATE_PYCBC_M_SEG_%s.npy' %(psd_segment_length), h[ifo])\n",
    "\n",
    "    elif psd_method =='losc':\n",
    "        \n",
    "        # Save a copy of the strain to disk\n",
    "        np.save('Results/'+'%s' % (ifo) + '_STRAIN_LOSC_M_SEG_%s.npy' %(psd_segment_length), strain[ifo])\n",
    "        \n",
    "        # save a copy of the residual strain to disk\n",
    "        np.save('Results/'+'%s' % (ifo) + '_RESIDUAL_LOSC_M_SEG_%s.npy' %(psd_segment_length), wt[ifo])\n",
    "        \n",
    "        # save a copy of the template to disk\n",
    "        np.save('Results/'+'%s' % (ifo) + '_TEMPLATE_LOSC_M_SEG_%s.npy' %(psd_segment_length), h[ifo])\n",
    "\n",
    "# Save a copy of time\n",
    "t = np.arange(0,len(strain['H'])/4096.,1./4096.)\n",
    "np.save('Results/sample_times.npy', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSC GW Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:04:44.186866Z",
     "start_time": "2018-12-03T12:04:44.128058Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Loading data')\n",
    "eventname = 'GW150914'\n",
    "events = json.load(open(\"BBH_events_v3.json\",\"r\"))\n",
    "event = events[eventname]\n",
    "fn_H1 = event['fn_H1']              # File name for H1 data\n",
    "fn_L1 = event['fn_L1']              # File name for L1 data\n",
    "fn_template = event['fn_template']  # File name for template waveform\n",
    "fs = event['fs']                    # Set sampling rate\n",
    "tevent = event['tevent']            # Set approximate event GPS time\n",
    "fband = event['fband']              # frequency band for bandpassing signal\n",
    "\n",
    "# read in data from H1 and L1, if available:\n",
    "strain_H1, time_H1, chan_dict_H1 = rl.loaddata(fn_H1, 'H1')\n",
    "strain_L1, time_L1, chan_dict_L1 = rl.loaddata(fn_L1, 'L1')\n",
    "strain_H1 = strain_H1*win\n",
    "strain_L1 = strain_L1*win\n",
    "\n",
    "# resave as .npy.\n",
    "np.save('Results/H_STRAIN_LOSC_UNFILT_SEG_%s.npy' %(psd_segment_length), np.vstack((time_H1,strain_H1)).T)\n",
    "np.save('Results/L_STRAIN_LOSC_UNFILT_SEG_%s.npy' %(psd_segment_length), np.vstack((time_L1,strain_L1)).T)\n",
    "\n",
    "\n",
    "# both H1 and L1 will have the same time vector, so:\n",
    "time = time_H1\n",
    "# the time sample interval (uniformly sampled!)\n",
    "dt = time[1] - time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:04:44.630836Z",
     "start_time": "2018-12-03T12:04:44.608607Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Estimate PSDs')\n",
    "# number of sample for the fast fourier transform:\n",
    "NFFT = psd_segment_length*fs\n",
    "Pxx_H1, freqs = mlab.psd(strain_H1, Fs = fs, NFFT = NFFT)\n",
    "Pxx_L1, freqs = mlab.psd(strain_L1, Fs = fs, NFFT = NFFT)\n",
    "\n",
    "# We will use interpolations of the ASDs computed above for whitening:\n",
    "psd_H1 = interp1d(freqs, Pxx_H1)\n",
    "psd_L1 = interp1d(freqs, Pxx_L1)\n",
    "\n",
    "# Here is an approximate, smoothed PSD for H1 during O1, with no lines. We'll use it later.    \n",
    "Pxx = (1.e-22*(18./(0.1+freqs))**2)**2+0.7e-23**2+((freqs/2000.)*4.e-23)**2\n",
    "psd_smooth = interp1d(freqs, Pxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:04:45.095535Z",
     "start_time": "2018-12-03T12:04:45.049801Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function to whiten data\n",
    "print(\"Whitening data\")\n",
    "def whiten(strain, interp_psd, dt):\n",
    "    Nt = len(strain)\n",
    "    freqs = rfftfreq(Nt, dt)\n",
    "    \n",
    "    hf = np.fft.rfft(strain)\n",
    "    psd = interp_psd(freqs)\n",
    "    white_hf = hf / np.sqrt(psd)\n",
    "    white_ht = np.fft.irfft(white_hf, n=Nt)\n",
    "    return white_ht, psd\n",
    "\n",
    "bb, ab = butter(4, [fband[0]*2./fs, fband[1]*2./fs], btype='band')\n",
    "normalization = np.sqrt((fband[1]-fband[0])/(fs/2))\n",
    "\n",
    "whiten_data = 0\n",
    "if whiten_data:\n",
    "    # now whiten the data from H1 and L1, and the template (use H1 PSD):\n",
    "    strain_H1_whiten = whiten(strain_H1,psd_H1,dt)\n",
    "    strain_L1_whiten = whiten(strain_L1,psd_L1,dt)\n",
    "    \n",
    "    # We need to suppress the high frequency noise (no signal!) with some bandpassing:\n",
    "    strain_H1_whitenbp = filtfilt(bb, ab, strain_H1_whiten) / normalization\n",
    "    strain_L1_whitenbp = filtfilt(bb, ab, strain_L1_whiten) / normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:04:45.565248Z",
     "start_time": "2018-12-03T12:04:45.527300Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# read in the template (plus and cross) and parameters for the theoretical waveform\n",
    "print(\"Read in and whiten template.\")\n",
    "f_template = h5py.File(fn_template, \"r\")\n",
    "\n",
    "# extract metadata from the template file:\n",
    "template_p, template_c = f_template[\"template\"][...]\n",
    "t_m1 = f_template[\"/meta\"].attrs['m1']\n",
    "t_m2 = f_template[\"/meta\"].attrs['m2']\n",
    "t_a1 = f_template[\"/meta\"].attrs['a1']\n",
    "t_a2 = f_template[\"/meta\"].attrs['a2']\n",
    "t_approx = f_template[\"/meta\"].attrs['approx']\n",
    "f_template.close()\n",
    "\n",
    "# the template extends to roughly 16s, zero-padded to the 32s data length. The merger will be roughly 16s in.\n",
    "template_offset = 16\n",
    "\n",
    "# whiten the templates:\n",
    "template_p_whiten,_ = whiten(template_p,psd_H1,dt)\n",
    "template_c_whiten,_ = whiten(template_c,psd_H1,dt)\n",
    "template_p_whitenbp = filtfilt(bb, ab, template_p_whiten) / normalization\n",
    "template_c_whitenbp = filtfilt(bb, ab, template_c_whiten) / normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:04:46.177955Z",
     "start_time": "2018-12-03T12:04:46.133408Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -- To calculate the PSD of the data, choose an overlap and a window (common to all detectors)\n",
    "#   that minimizes \"spectral leakage\" https://en.wikipedia.org/wiki/Spectral_leakage\n",
    "NFFT = psd_segment_length*fs\n",
    "psd_window = np.blackman(NFFT)\n",
    "# and a 50% overlap:\n",
    "NOVL = NFFT/2\n",
    "\n",
    "# define the complex template, common to both detectors:\n",
    "template = (template_p + template_c*1.j) \n",
    "# We will record the time where the data match the END of the template.\n",
    "etime = time+template_offset\n",
    "# the length and sampling rate of the template MUST match that of the data.\n",
    "datafreq = np.fft.fftfreq(template.size)*fs\n",
    "df = np.abs(datafreq[1] - datafreq[0])\n",
    "\n",
    "# to remove effects at the beginning and end of the data stretch, window the data\n",
    "# https://en.wikipedia.org/wiki/Window_function#Tukey_window\n",
    "dwindow = tukey(template.size, alpha=1./8)  # Tukey window preferred, but requires recent scipy version \n",
    "\n",
    "# prepare the template fft.\n",
    "template_fft = np.fft.fft(template*dwindow) / fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:04:46.847512Z",
     "start_time": "2018-12-03T12:04:46.678599Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# loop over the detectors\n",
    "dets = ['H1', 'L1']\n",
    "for det in dets:\n",
    "\n",
    "    if det is 'L1': data = strain_L1.copy()\n",
    "    else:           data = strain_H1.copy()\n",
    "\n",
    "    # -- Calculate the PSD of the data.  Also use an overlap, and window:\n",
    "    data_psd, freqs = mlab.psd(data, Fs = fs, NFFT = NFFT, window=psd_window, noverlap=NOVL) #, noverlap=NOVL\n",
    "    \n",
    "    if not whiten_data:\n",
    "        print('Whitening using match psd.')\n",
    "        data_psd_interp = interp1d(freqs, data_psd)\n",
    "        if det == 'H1':\n",
    "            strain_H1_whiten, psd_H1_correct = whiten(strain_H1,data_psd_interp,dt)\n",
    "            strain_H1_whitenbp = filtfilt(bb, ab, strain_H1_whiten) / normalization\n",
    "        else:\n",
    "            strain_L1_whiten, psd_L1_correct = whiten(strain_L1,psd_L1,dt)\n",
    "            strain_L1_whitenbp = filtfilt(bb, ab, strain_L1_whiten) / normalization\n",
    "\n",
    "    # Take the Fourier Transform (FFT) of the data and the template (with dwindow)\n",
    "    data_fft = np.fft.fft(data*dwindow) / fs\n",
    "\n",
    "    # -- Interpolate to get the PSD values at the needed frequencies\n",
    "    power_vec = np.interp(np.abs(datafreq), freqs, data_psd)\n",
    "\n",
    "    # -- Calculate the matched filter output in the time domain:\n",
    "    # Multiply the Fourier Space template and data, and divide by the noise power in each frequency bin.\n",
    "    # Taking the Inverse Fourier Transform (IFFT) of the filter output puts it back in the time domain,\n",
    "    # so the result will be plotted as a function of time off-set between the template and the data:\n",
    "    optimal = data_fft * template_fft.conjugate() / power_vec\n",
    "    optimal_time = 2*np.fft.ifft(optimal)*fs\n",
    "\n",
    "    # -- Normalize the matched filter output:\n",
    "    # Normalize the matched filter output so that we expect a value of 1 at times of just noise.\n",
    "    # Then, the peak of the matched filter output will tell us the signal-to-noise ratio (SNR) of the signal.\n",
    "    sigmasq = 1*(template_fft * template_fft.conjugate() / power_vec).sum() * df\n",
    "    sigma = np.sqrt(np.abs(sigmasq))\n",
    "    SNR_complex = optimal_time/sigma\n",
    "\n",
    "    # shift the SNR vector by the template length so that the peak is at the END of the template\n",
    "    peaksample = int(data.size / 2)  # location of peak in the template\n",
    "    SNR_complex = np.roll(SNR_complex,peaksample)\n",
    "    SNR = abs(SNR_complex)\n",
    "\n",
    "    # find the time and SNR value at maximum:\n",
    "    indmax = np.argmax(SNR)\n",
    "    timemax = time[indmax]\n",
    "    SNRmax = SNR[indmax]\n",
    "\n",
    "    # Calculate the \"effective distance\" (see FINDCHIRP paper for definition)\n",
    "    # d_eff = (8. / SNRmax)*D_thresh\n",
    "    d_eff = sigma / SNRmax\n",
    "    # -- Calculate optimal horizon distnace\n",
    "    horizon = sigma/8\n",
    "\n",
    "    # Extract time offset and phase at peak\n",
    "    phase = np.angle(SNR_complex[indmax])\n",
    "    offset = (indmax-peaksample)\n",
    "\n",
    "    # apply time offset, phase, and d_eff to template \n",
    "    template_phaseshifted = np.real(template*np.exp(1j*phase))    # phase shift the template\n",
    "    template_rolled = np.roll(template_phaseshifted,offset) / d_eff  # Apply time offset and scale amplitude\n",
    "    \n",
    "    # Whiten and band-pass the template for plotting\n",
    "    template_whitened,_ = whiten(template_rolled,interp1d(freqs, data_psd),dt)  # whiten the template\n",
    "    template_match = filtfilt(bb, ab, template_whitened) / normalization # Band-pass the template\n",
    "    \n",
    "    # Save template and strain\n",
    "    if det == 'L1':\n",
    "        np.save('Results/L_STRAIN_LOSC_SEG_%s.npy' %(psd_segment_length), strain_L1_whitenbp)\n",
    "        np.save('Results/L_TEMPLATE_LOSC_SEG_%s.npy' %(psd_segment_length), template_match)\n",
    "        np.save('Results/L_RESIDUAL_LOSC_SEG_%s.npy' %(psd_segment_length), strain_L1_whitenbp-template_match)\n",
    "        np.save('Results/L_PSD_LOSC_SEG_%s.npy' %(psd_segment_length), psd_L1_correct)\n",
    "    else:\n",
    "        np.save('Results/H_STRAIN_LOSC_SEG_%s.npy' %(psd_segment_length), strain_H1_whitenbp)\n",
    "        np.save('Results/H_TEMPLATE_LOSC_SEG_%s.npy' %(psd_segment_length), template_match)\n",
    "        np.save('Results/H_RESIDUAL_LOSC_SEG_%s.npy' %(psd_segment_length), strain_H1_whitenbp-template_match)\n",
    "        np.save('Results/H_PSD_LOSC_SEG_%s.npy' %(psd_segment_length), psd_H1_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RW CC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:54:15.817753Z",
     "start_time": "2018-12-03T13:54:15.727920Z"
    },
    "code_folding": [
     2,
     59,
     79,
     91
    ]
   },
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def rw_cor(time, signal, template, sr=4096, dt=0.21, step=0.01):\n",
    "    \"\"\"Calculates the running-window cross correlation between signal and\n",
    "    template with window size dt and step size step.\"\"\"\n",
    "    \n",
    "    # Initialize arrays and vals\n",
    "    \n",
    "    #time = np.round(time,30)\n",
    "    t_i = time[0]-time[0]\n",
    "    t_f = time[len(time)-1]-time[0]\n",
    "    \n",
    "    t1 = np.array([])\n",
    "    t2 = np.array([])\n",
    "    t = np.array([])\n",
    "    i_sta = np.array([])\n",
    "    i_sto = np.array([])\n",
    "    cc = np.array([])\n",
    "    \n",
    "    t_1 = t_i\n",
    "    t_2 = t_i + dt\n",
    "    \n",
    "    while t_i < t_f:\n",
    "        \n",
    "        # Slices signal and template into interval between t_1 and t_2\n",
    "        # Convert slices to mpf.\n",
    "        #indeces = np.where(np.logical_and(time>=t_1, time<=t_2))\n",
    "        i_start = int(np.round(t_1*sr)) #int(indeces[0][0])\n",
    "        i_stop = int(np.round(t_2*sr))+1 #int(indeces[0][-1])+1\n",
    "        sgnl_int = signal[i_start:i_stop]\n",
    "        tmpl_int = template[i_start:i_stop]\n",
    "        \n",
    "        # Calculate window position and cross correlation of signal and\n",
    "        # template intervals.\n",
    "        mid_time = (t_2+t_1)/2\n",
    "        sgnl_int = sgnl_int-np.mean(sgnl_int)\n",
    "        tmpl_int = tmpl_int-np.mean(tmpl_int)\n",
    "        num = np.sum(sgnl_int*tmpl_int)\n",
    "        den = np.sqrt(np.sum(sgnl_int**2)*np.sum(tmpl_int**2))\n",
    "        if den == 0:\n",
    "            cor = 0\n",
    "        else:\n",
    "            cor = num/den\n",
    "            #cor = np.sum(sgnl_int*tmpl_int)/np.sqrt(np.sum(np.square(sgnl_int))\\\n",
    "                         #*np.sum(np.square(tmpl_int)))\n",
    "        \n",
    "        # Append values to defined arrays, repeat loop.\n",
    "        t = np.append(t,mid_time)\n",
    "        t1 = np.append(t1,t_1)\n",
    "        t2 = np.append(t2,t_2)\n",
    "        i_sta = np.append(i_sta, i_start)\n",
    "        i_sto = np.append(i_sto, i_stop)\n",
    "        cc = np.append(cc, cor)\n",
    "        t_1 += step\n",
    "        t_2 += step\n",
    "        t_i = t_2\n",
    "    \n",
    "    return cc, t#, t1, t2, i_sta, i_sto\n",
    "\n",
    "def running_avg(x, n):\n",
    "    \"\"\" Running window average using 2n+1 data points to calculate average.\n",
    "    \"\"\"\n",
    "\n",
    "    ravg = np.array([])\n",
    "    \n",
    "    # Add n/2 padding to each side of data. Use n/2 mirrored data-points\n",
    "    # around i = o and i = len(x). n must be whole number\n",
    "    stop = int(n/2)+1\n",
    "    \n",
    "    x = np.concatenate((np.flip(x[1:stop],-1),x))\n",
    "    x = np.append(x,np.flip(x[-stop:len(x)-1],-1))\n",
    "    \n",
    "    # Calculate running average\n",
    "    size = len(x)\n",
    "    for i in range(size-n+1):\n",
    "            avg = np.mean(x[i:n+i])\n",
    "            ravg = np.append(ravg,avg)\n",
    "    return ravg\n",
    "\n",
    "def hist(data, no_bins = 0, pdf = True, title=\"\",lbl=\"\", color='k'):\n",
    "    if no_bins == 0:\n",
    "        no_bins = round(len(data)/3)\n",
    "        print(\"No. of bins in '\"+ title+\"':\")\n",
    "        print(no_bins)\n",
    "    values, bins = np.histogram(data, no_bins, density = pdf)\n",
    "    center = (bins[:-1]+bins[1:])/2\n",
    "    plt.plot(center,values,label=lbl,c=color)\n",
    "    plt.title(title)\n",
    "    if pdf == True:\n",
    "        plt.ylabel(\"PDF\")\n",
    "\n",
    "def kullback_leibler(dist, model):\n",
    "    vals = []\n",
    "    dist = dist/np.sum(dist)\n",
    "    model = model/np.sum(model)\n",
    "    for i,j in zip(dist, model):\n",
    "        if i == 0:\n",
    "            val_tmp = 0\n",
    "        else:\n",
    "            val_tmp = i*np.log(i/j)\n",
    "        vals.append(val_tmp)\n",
    "    theta = np.sum(vals)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:54:08.094899Z",
     "start_time": "2018-12-03T13:54:08.045095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import PyCBC data\n",
    "\n",
    "HS_PYCBC_M = np.load('Results/H_STRAIN_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "LS_PYCBC_M = np.load('Results/L_STRAIN_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "HT_PYCBC_M = np.load('Results/H_TEMPLATE_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "LT_PYCBC_M = np.load('Results/L_TEMPLATE_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "HR_PYCBC_M = np.load('Results/H_RESIDUAL_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "LR_PYCBC_M = np.load('Results/L_RESIDUAL_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "H_PSD_PYCBC_M = np.load('Results/H_PSD_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "L_PSD_PYCBC_M = np.load('Results/L_PSD_PYCBC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "\n",
    "\n",
    "HS_LOSC_M = np.load('Results/H_STRAIN_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "LS_LOSC_M = np.load('Results/L_STRAIN_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "HT_LOSC_M = np.load('Results/H_TEMPLATE_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "LT_LOSC_M = np.load('Results/L_TEMPLATE_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "HR_LOSC_M = np.load('Results/H_RESIDUAL_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "LR_LOSC_M = np.load('Results/L_RESIDUAL_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "H_PSD_LOSC_M = np.load('Results/H_PSD_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "L_PSD_LOSC_M = np.load('Results/L_PSD_LOSC_M_SEG_%s.npy' %(psd_segment_length))\n",
    "\n",
    "\n",
    "# Import LOSC data\n",
    "\n",
    "HS_LOSC = np.load('Results/H_STRAIN_LOSC_SEG_%s.npy' %(psd_segment_length)), \n",
    "LS_LOSC = np.load('Results/L_STRAIN_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "HT_LOSC = np.load('Results/H_TEMPLATE_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "LT_LOSC = np.load('Results/L_TEMPLATE_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "HR_LOSC = np.load('Results/H_RESIDUAL_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "LR_LOSC = np.load('Results/L_RESIDUAL_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "H_PSD_LOSC = np.load('Results/H_PSD_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "L_PSD_LOSC = np.load('Results/L_PSD_LOSC_SEG_%s.npy' %(psd_segment_length))\n",
    "\n",
    "# Import Hao's data\n",
    "\n",
    "HR_WBP = np.load('Results/4096Hz_psd4_type2_H_residual_wbp.npy')\n",
    "HT_WBP = np.load('Results/4096Hz_psd4_type2_H_tpl_match_wbp.npy')\n",
    "\n",
    "LR_WBP = np.load('Results/4096Hz_psd4_type2_L_residual_wbp.npy')\n",
    "LT_WBP = np.load('Results/4096Hz_psd4_type2_L_tpl_match_wbp.npy')\n",
    "\n",
    "# Import time\n",
    "\n",
    "t_pycbc = np.load('Results/sample_times.npy')\n",
    "t_losc = t_pycbc#[i_s:-i_s]\n",
    "t_wbp = np.arange(0,32,1./4096.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T13:54:19.747035Z",
     "start_time": "2018-12-03T13:54:18.035252Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Running window CC analysis\n",
    "# PyCBC\n",
    "\n",
    "CCH_PYCBC_M, pos_PYCBC_M = rw_cor(t_pycbc, HR_PYCBC_M, HT_PYCBC_M)\n",
    "CCL_PYCBC_M, pos_PYCBC_M = rw_cor(t_pycbc, LR_PYCBC_M, LT_PYCBC_M)\n",
    "CCH_PYCBC_M = running_avg(CCH_PYCBC_M, 51)\n",
    "CCL_PYCBC_M = running_avg(CCL_PYCBC_M, 51)\n",
    "\n",
    "\n",
    "CCH_LOSC_M, pos_LOSC_M = rw_cor(t_pycbc, HR_LOSC_M, HT_LOSC_M)\n",
    "CCL_LOSC_M, pos_LOSC_M = rw_cor(t_pycbc, LR_LOSC_M, LT_LOSC_M)\n",
    "CCH_LOSC_M = running_avg(CCH_LOSC_M, 51)\n",
    "CCL_LOSC_M = running_avg(CCL_LOSC_M, 51)\n",
    "\n",
    "# LOSC\n",
    "\n",
    "CCH_LOSC, pos_LOSC = rw_cor(t_losc, HR_LOSC, HT_LOSC)\n",
    "CCL_LOSC, pos_LOSC = rw_cor(t_losc, LR_LOSC, LT_LOSC)\n",
    "CCH_LOSC = running_avg(CCH_LOSC, 51)\n",
    "CCL_LOSC = running_avg(CCL_LOSC, 51)\n",
    "\n",
    "# Hao\n",
    "\n",
    "CCH_WBP, pos_WBPH = rw_cor(t_wbp, HR_WBP, HT_WBP)\n",
    "CCL_WBP, pos_WBPL = rw_cor(t_wbp, LR_WBP, LT_WBP)\n",
    "CCH_WBP = running_avg(CCH_LOSC, 51)\n",
    "CCL_WBP = running_avg(CCL_LOSC, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:19:32.599609Z",
     "start_time": "2018-12-03T12:19:32.345914Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot running window CC analysis\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.plot(pos_PYCBC_M, CCH_PYCBC_M, c='k', label='Hanford')\n",
    "plt.plot(pos_PYCBC_M, CCL_PYCBC_M, c='r', label='Livingston')\n",
    "plt.xlabel('Window position [s]')\n",
    "plt.ylabel('Cross correlation')\n",
    "#plt.title('RW CC PYCBC w. PYCBC method')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/CC_PYCBC_M_SEG_%s_mean.eps' %(psd_segment_length))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(pos_LOSC_M, CCH_LOSC_M, c='k', label='Hanford')\n",
    "plt.plot(pos_LOSC_M, CCL_LOSC_M, c='r', label='Livingston')\n",
    "plt.xlabel('Window position [s]')\n",
    "plt.ylabel('Cross correlation')\n",
    "#plt.title('RW CC PYCBC w. LOSC method')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/CC_LOSC_M_SEG_%s.eps' %(psd_segment_length))\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot(pos_LOSC, CCH_LOSC, c='k', label='Hanford')\n",
    "plt.plot(pos_LOSC, CCL_LOSC, c='r', label='Livingston')\n",
    "plt.xlabel('Window position [s]')\n",
    "plt.ylabel('Cross correlation')\n",
    "#plt.title('RW CC LOSC')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/CC_LOSC_SEG_%s.eps' %(psd_segment_length))\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T12:46:57.162585Z",
     "start_time": "2018-12-03T12:46:56.964866Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare running window CC\n",
    "plt.figure(4)\n",
    "plt.plot(pos_LOSC, CCH_LOSC, c='k', label='LOSC')\n",
    "plt.plot(pos_PYCBC_M, CCH_PYCBC_M, c='r', label='PyCBC')\n",
    "#plt.plot(pos_WBPH, CCH_WBP, c='r', label=\"Hao's script\")\n",
    "plt.xlabel('Window position [s]')\n",
    "plt.ylabel('Cross correlation')\n",
    "plt.title('RW CC w. PyCBC settings for Hanford')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/CCH_test_6_orig.eps')\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(pos_LOSC, CCL_LOSC, c='k', label='LOSC')\n",
    "plt.plot(pos_PYCBC_M, CCL_PYCBC_M, c='r', label='PyCBC')\n",
    "#plt.plot(pos_WBPH, CCL_WBP, c='r', label=\"Hao's script\")\n",
    "plt.xlabel('Window position [s]')\n",
    "plt.ylabel('Cross correlation')\n",
    "plt.title('RW CC w. PyCBC settings for Livingston')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/CCL_test_6_orig.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T12:24:59.834054Z",
     "start_time": "2018-11-29T12:24:59.808740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fourier spectrum of whitened strain\n",
    "\n",
    "f1 = rfftfreq(len(HS_PYCBC_M), 1./4096.)\n",
    "f2 = H_PSD_PYCBC_M[:,0]\n",
    "f3 = rfftfreq(len(HS_LOSC), 1./4096.)\n",
    "\n",
    "# PyCBC\n",
    "\n",
    "FFT_H_PYCBC_M = np.abs(rfft(HS_PYCBC_M))\n",
    "FFT_L_PYCBC_M = np.abs(rfft(LS_PYCBC_M))\n",
    "\n",
    "FFT_H_LOSC_M = np.abs(rfft(HS_LOSC_M))\n",
    "FFT_L_LOSC_M = np.abs(rfft(LS_LOSC_M))\n",
    "\n",
    "# LOSC\n",
    "FFT_H_LOSC = np.abs(rfft(HS_LOSC))\n",
    "FFT_L_LOSC = np.abs(rfft(LS_LOSC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T12:27:19.812335Z",
     "start_time": "2018-11-29T12:27:18.832155Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.loglog(f1, FFT_H_PYCBC_M, c='k', label='Hanford')\n",
    "plt.loglog(f1, FFT_L_PYCBC_M, c='r', label='Livingston')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.xlim(10,400)\n",
    "plt.savefig('Plots/FFT_PYCBC_M_SEG_%s.eps' %(psd_segment_length))\n",
    "\n",
    "plt.figure(5)\n",
    "plt.loglog(f1, FFT_H_LOSC_M, c='k', label='Hanford')\n",
    "plt.loglog(f1, FFT_L_LOSC_M, c='r', label='Livingston')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.xlim(10,400)\n",
    "plt.savefig('Plots/FFT_LOSC_M_SEG_%s.eps' %(psd_segment_length))\n",
    "\n",
    "plt.figure(6)\n",
    "plt.loglog(f3, FFT_H_LOSC, c='k', label='Hanford')\n",
    "plt.loglog(f3, FFT_L_LOSC, c='r', label='Livingston')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.xlim(10,400)\n",
    "plt.savefig('Plots/FFT_LOSC_SEG_%s.eps' %(psd_segment_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T12:27:27.450078Z",
     "start_time": "2018-11-29T12:27:26.714014Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(7)\n",
    "plt.loglog(f2, H_PSD_PYCBC_M[:,1], c='k', label='Hanford')\n",
    "plt.loglog(f2, L_PSD_PYCBC_M[:,1], c='r', label='Livingston')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "plt.xlim(10,400)\n",
    "plt.savefig('Plots/PSD_PYCBC_M_SEG_%s.eps' %(psd_segment_length))\n",
    "\n",
    "plt.figure(8)\n",
    "plt.loglog(f2, H_PSD_LOSC_M[:,1], c='k', label='Hanford')\n",
    "plt.loglog(f2, L_PSD_LOSC_M[:,1], c='r', label='Livingston')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "plt.xlim(10,400)\n",
    "plt.savefig('Plots/PSD_LOSC_M_SEG_%s.eps' %(psd_segment_length))\n",
    "\n",
    "plt.figure(9)\n",
    "plt.loglog(f2, H_PSD_LOSC, c='k', label='Hanford')\n",
    "plt.loglog(f2, L_PSD_LOSC, c='r', label='Livingston')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "plt.xlim(10,400)\n",
    "plt.savefig('Plots/PSD_LOSC_SEG_%s.eps' %(psd_segment_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
